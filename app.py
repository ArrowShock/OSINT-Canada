import streamlit as st
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse, unquote, quote
import io
import zipfile
import urllib3
import time
import pandas as pd

# å±è”½è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
st.set_page_config(page_title="OSINT ä¸‹è½½å™¨ (V23 ä¿®å¤ç‰ˆ)", layout="wide", page_icon="ğŸ•µï¸")

# --- è¾…åŠ©å‡½æ•° ---
def get_ext(filename):
    base, ext = os.path.splitext(filename)
    if not ext: return ".unknown"
    return ext.lower()

def is_target_file(href):
    # å®½æ¾æ£€æŸ¥ï¼šåªè¦åŒ…å«æŒ‡å®šåç¼€å³å¯
    valid = ['.pdf', '.xlsx', '.xls', '.csv', '.docx', '.doc', '.zip']
    return any(href.lower().endswith(ext) for ext in valid)

def get_file_size_mb(url):
    try:
        # V23 ä¿®å¤ï¼šè¯·æ±‚å‰å…ˆå¤„ç† URL ä¸­çš„ç©ºæ ¼
        safe_url = url.replace(" ", "%20")
        r = requests.head(safe_url, verify=False, timeout=5)
        return int(r.headers.get('Content-Length', 0)) / (1024 * 1024)
    except: return 0

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ•µï¸ OSINT ä¸‹è½½å™¨ (V23 ç©ºæ ¼ä¿®å¤ç‰ˆ)")
st.caption("æ”¯æŒï¼šè‡ªåŠ¨æ‰«æç½‘é¡µ / æ‰‹åŠ¨ç²˜è´´é“¾æ¥åˆ—è¡¨")

if 'found_files' not in st.session_state: st.session_state['found_files'] = []

# === é€‰é¡¹å¡ ===
tab1, tab2 = st.tabs(["ğŸ”— æ¨¡å¼ä¸€ï¼šè‡ªåŠ¨æ‰«æç½‘é¡µ", "ğŸ“‹ æ¨¡å¼äºŒï¼šç²˜è´´é“¾æ¥åˆ—è¡¨"])

with tab1:
    target_url = st.text_input("ç›®æ ‡ç½‘å€", placeholder="https://...")
    if st.button("ğŸš€ æ‰«æç½‘é¡µ", key="btn_scan"):
        if target_url:
            try:
                with st.spinner("æ‰«æä¸­..."):
                    headers = {"User-Agent": "Mozilla/5.0"}
                    r = requests.get(target_url, headers=headers, verify=False)
                    soup = BeautifulSoup(r.text, 'html.parser')
                    files = []
                    for a in soup.find_all('a', href=True):
                        if is_target_file(a['href']):
                            full_url = urljoin(target_url, a['href'])
                            name = os.path.basename(unquote(urlparse(full_url).path))
                            if not any(f['URL'] == full_url for f in files):
                                files.append({"ä¸‹è½½?": False, "åºå·": len(files)+1, "æ–‡ä»¶å": name, "URL": full_url})
                    st.session_state['found_files'] = files
                    st.success(f"æ‰«æå®Œæˆï¼å‘ç° {len(files)} ä¸ªæ–‡ä»¶")
            except Exception as e: st.error(str(e))

with tab2:
    st.info("ğŸ’¡ æç¤ºï¼šå°† Link Gopher æå–çš„é“¾æ¥ç²˜è´´åˆ°ä¸‹æ–¹ã€‚æ”¯æŒå¸¦ç©ºæ ¼çš„ URLã€‚")
    raw_text = st.text_area("åœ¨æ­¤ç²˜è´´é“¾æ¥ (æ¯è¡Œä¸€ä¸ª)", height=150)
    
    if st.button("ğŸ” è§£æé“¾æ¥", key="btn_parse"):
        if raw_text:
            # === V23 æ ¸å¿ƒä¿®å¤ï¼šæ”¹ç”¨ splitlines() æŒ‰è¡Œåˆ‡å‰²ï¼Œä¿æŠ¤ç©ºæ ¼ ===
            lines = raw_text.splitlines()
            files = []
            for line in lines:
                line = line.strip() # å»é™¤é¦–å°¾ä¸å¯è§å­—ç¬¦
                if not line: continue
                
                # åªè¦è¿™è¡Œæ–‡å­—é‡Œæœ‰ http å’Œ .pdf å°±å¯ä»¥
                if "http" in line and is_target_file(line):
                    # æå– URL (å‡è®¾æ•´è¡Œå°±æ˜¯ URL)
                    # å¦‚æœæœ‰å‰ç¼€æ‚è´¨ï¼Œå°è¯•å®šä½ http
                    http_pos = line.find("http")
                    clean_url = line[http_pos:]
                    
                    # æå–æ–‡ä»¶å
                    try:
                        name = os.path.basename(unquote(urlparse(clean_url).path))
                    except:
                        name = "unknown_file.pdf"
                        
                    if not any(f['URL'] == clean_url for f in files):
                        files.append({"ä¸‹è½½?": False, "åºå·": len(files)+1, "æ–‡ä»¶å": name, "URL": clean_url})
            
            st.session_state['found_files'] = files
            if files:
                st.success(f"æˆåŠŸè§£æå‡º {len(files)} ä¸ªæ–‡ä»¶ï¼è¯·åœ¨ä¸‹æ–¹ä¸‹è½½ã€‚")
            else:
                st.warning("æœªå‘ç°æœ‰æ•ˆé“¾æ¥ã€‚è¯·ç¡®è®¤ç²˜è´´å†…å®¹æ¯è¡ŒåŒ…å«ä¸€ä¸ª http...pdf é“¾æ¥ã€‚")

# --- é€šç”¨ä¸‹è½½åŒº ---
if st.session_state['found_files']:
    st.markdown("---")
    st.subheader(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½ ({len(st.session_state['found_files'])} ä¸ªæ–‡ä»¶)")
    
    # åŒºé—´é€‰æ‹©
    c1, c2, c3, c4 = st.columns([1,1,2,2])
    with c1: start = st.number_input("èµ·å§‹", 1, value=1)
    with c2: end = st.number_input("ç»“æŸ", 1, value=len(st.session_state['found_files']))
    
    if c3.button("âœ… é€‰ä¸­æ­¤èŒƒå›´"):
        for f in st.session_state['found_files']:
            f['ä¸‹è½½?'] = (start <= f['åºå·'] <= end)
    
    if c4.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰"):
        for f in st.session_state['found_files']: f['ä¸‹è½½?'] = False

    # è¡¨æ ¼
    df = pd.DataFrame(st.session_state['found_files'])
    edited_df = st.data_editor(
        df, 
        height=400, 
        key="editor", 
        hide_index=True, 
        column_config={"URL": st.column_config.LinkColumn()}
    )
    
    # ä¸‹è½½é€»è¾‘
    selected = edited_df[edited_df["ä¸‹è½½?"] == True]
    count = len(selected)
    
    if st.button(f"ğŸ“¦ å¼€å§‹ä¸‹è½½ ({count} ä¸ªæ–‡ä»¶)", type="primary"):
        if count > 0:
            zip_buffer = io.BytesIO()
            progress_text = st.empty()
            my_bar = st.progress(0)
            
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                total = len(selected)
                for i, (index, row) in enumerate(selected.iterrows()):
                    try:
                        progress_text.text(f"æ­£åœ¨ä¸‹è½½ ({i+1}/{total}): {row['æ–‡ä»¶å']}")
                        headers = {"User-Agent": "Mozilla/5.0"}
                        
                        # === V23 ä¿®å¤ï¼šä¸‹è½½æ—¶è‡ªåŠ¨æŠŠç©ºæ ¼è½¬ä¸º %20 ===
                        download_url = row['URL'].replace(" ", "%20")
                        
                        sz = get_file_size_mb(download_url)
                        if sz > 100: 
                            st.toast(f"è·³è¿‡å¤§æ–‡ä»¶: {row['æ–‡ä»¶å']}", icon="âš ï¸")
                            continue
                            
                        r = requests.get(download_url, headers=headers, verify=False, timeout=60)
                        zf.writestr(row['æ–‡ä»¶å'], r.content)
                        my_bar.progress((i + 1) / total)
                    except Exception as e: 
                        print(e)
            
            my_bar.empty()
            progress_text.text("âœ… æ‰“åŒ…å®Œæˆï¼")
            st.download_button("ğŸš€ ä¿å­˜ ZIP", zip_buffer.getvalue(), "Epstein_Files.zip", "application/zip", type="primary")
