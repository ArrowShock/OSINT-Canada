import streamlit as st
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse, unquote, quote
import io
import zipfile
import urllib3
import time
import pandas as pd

# å±è”½è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
st.set_page_config(page_title="OSINT ä¸‹è½½å™¨ (V25 é€šè¡Œè¯ç‰ˆ)", layout="wide", page_icon="ğŸ•µï¸")

# --- è¾…åŠ©å‡½æ•° ---
def is_target_file(href):
    valid = ['.pdf', '.xlsx', '.xls', '.csv', '.docx', '.doc', '.zip']
    return any(href.lower().endswith(ext) for ext in valid)

def safe_encode_url(url):
    parts = urlparse(url)
    safe_path = quote(parts.path) 
    new_url = parts.scheme + "://" + parts.netloc + safe_path
    if parts.query: new_url += "?" + parts.query
    return new_url

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ•µï¸ OSINT ä¸‹è½½å™¨ (V25 é€šè¡Œè¯ç‰ˆ)")
st.caption("æ–°å¢ï¼šCookie æ³¨å…¥åŠŸèƒ½ï¼Œä¸“æ²» 'Access Denied' å¹´é¾„éªŒè¯é”")

if 'found_files' not in st.session_state: st.session_state['found_files'] = []

# === ä¾§è¾¹æ ï¼šé…ç½®åŒº ===
with st.sidebar:
    st.header("ğŸ” èº«ä»½ä¼ªè£…é…ç½®")
    st.info("å¦‚æœä¸‹è½½çš„ PDF æ‰“ä¸å¼€æˆ–åªæœ‰ 53KBï¼Œè¯·åœ¨æ­¤å¡«å…¥æµè§ˆå™¨çš„ Cookieã€‚")
    user_cookie = st.text_area("_ga=GA1.1.739891733.1767747350; nmstat=b851bd0d-2a3d-eccb-42fb-3f623f20f0b6; _ga_CSLL4ZEK4L=GS2.1.s1769920583$o4$g1$t1769922779$j2$l0$h0; QueueITAccepted-SDFrts345E-V3_usdojsearch=EventId%3Dusdojsearch%26RedirectType%3Dsafetynet%26IssueTime%3D1769922786%26Hash%3D6f3152dc965a89ee6c7c9b80e49518dfd2450c0da055fec55395106f682b10d6; QueueITAccepted-SDFrts345E-V3_usdojfiles=EventId%3Dusdojfiles%26RedirectType%3Dsafetynet%26IssueTime%3D1769923479%26Hash%3Dcc591226f177c714d9387feee5b2e510964acb0adf11d1fda4be9e6047bc955a", height=150, placeholder="ä¾‹å¦‚: SSESSxxx=...; _ga=...")

# === é€‰é¡¹å¡ ===
tab1, tab2 = st.tabs(["ğŸ”— æ¨¡å¼ä¸€ï¼šè‡ªåŠ¨æ‰«æç½‘é¡µ", "ğŸ“‹ æ¨¡å¼äºŒï¼šç²˜è´´é“¾æ¥åˆ—è¡¨"])

with tab1:
    target_url = st.text_input("ç›®æ ‡ç½‘å€", placeholder="https://...")
    if st.button("ğŸš€ æ‰«æç½‘é¡µ"):
        if target_url:
            try:
                headers = {"User-Agent": "Mozilla/5.0"}
                if user_cookie: headers["Cookie"] = user_cookie # æ³¨å…¥ Cookie
                
                r = requests.get(target_url, headers=headers, verify=False)
                soup = BeautifulSoup(r.text, 'html.parser')
                files = []
                for a in soup.find_all('a', href=True):
                    if is_target_file(a['href']):
                        full_url = urljoin(target_url, a['href'])
                        name = os.path.basename(unquote(urlparse(full_url).path))
                        if not any(f['URL'] == full_url for f in files):
                            files.append({"ä¸‹è½½?": False, "åºå·": len(files)+1, "æ–‡ä»¶å": name, "URL": full_url})
                st.session_state['found_files'] = files
                st.success(f"æ‰«æå®Œæˆï¼å‘ç° {len(files)} ä¸ªæ–‡ä»¶")
            except Exception as e: st.error(str(e))

with tab2:
    st.info("ğŸ’¡ æç¤ºï¼šå°† Link Gopher æå–çš„é“¾æ¥ç²˜è´´åˆ°ä¸‹æ–¹ã€‚")
    raw_text = st.text_area("åœ¨æ­¤ç²˜è´´é“¾æ¥ (æ¯è¡Œä¸€ä¸ª)", height=150)
    
    if st.button("ğŸ” è§£æé“¾æ¥"):
        if raw_text:
            lines = raw_text.splitlines()
            files = []
            for line in lines:
                line = line.strip()
                if not line: continue
                if "http" in line and is_target_file(line):
                    http_pos = line.find("http")
                    clean_url = line[http_pos:]
                    try: name = os.path.basename(unquote(urlparse(clean_url).path))
                    except: name = "unknown_file.pdf"
                    if not any(f['URL'] == clean_url for f in files):
                        files.append({"ä¸‹è½½?": False, "åºå·": len(files)+1, "æ–‡ä»¶å": name, "URL": clean_url})
            st.session_state['found_files'] = files
            if files: st.success(f"æˆåŠŸè§£æ {len(files)} ä¸ªæ–‡ä»¶")
            else: st.warning("æœªå‘ç°æœ‰æ•ˆé“¾æ¥")

# --- ä¸‹è½½åŒº ---
if st.session_state['found_files']:
    st.markdown("---")
    st.subheader(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½ ({len(st.session_state['found_files'])} ä¸ªæ–‡ä»¶)")
    
    c1, c2, c3, c4 = st.columns([1,1,2,2])
    with c1: start = st.number_input("èµ·å§‹", 1, value=1)
    with c2: end = st.number_input("ç»“æŸ", 1, value=len(st.session_state['found_files']))
    if c3.button("âœ… é€‰ä¸­èŒƒå›´"):
        for f in st.session_state['found_files']: f['ä¸‹è½½?'] = (start <= f['åºå·'] <= end)
    if c4.button("ğŸ—‘ï¸ æ¸…ç©º"):
        for f in st.session_state['found_files']: f['ä¸‹è½½?'] = False

    df = pd.DataFrame(st.session_state['found_files'])
    edited_df = st.data_editor(df, height=400, key="editor", hide_index=True, column_config={"URL": st.column_config.LinkColumn()})
    
    selected = edited_df[edited_df["ä¸‹è½½?"] == True]
    count = len(selected)
    
    if st.button(f"ğŸ“¦ å¼€å§‹ä¸‹è½½ ({count} ä¸ªæ–‡ä»¶)", type="primary"):
        if count > 0:
            zip_buffer = io.BytesIO()
            progress_text = st.empty()
            my_bar = st.progress(0)
            
            success = 0
            fail = 0
            
            with zipfile.ZipFile(zip_buffer, "w") as zf:
                total = len(selected)
                for i, (index, row) in enumerate(selected.iterrows()):
                    try:
                        progress_text.text(f"ä¸‹è½½ä¸­: {row['æ–‡ä»¶å']}")
                        
                        # === V25 æ ¸å¿ƒï¼šå¸¦ç€ Cookie å»ä¸‹è½½ ===
                        download_url = safe_encode_url(row['URL'])
                        headers = {
                            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                            "Referer": "https://www.justice.gov/"
                        }
                        if user_cookie: headers["Cookie"] = user_cookie # <--- å…³é”®ï¼
                        
                        r = requests.get(download_url, headers=headers, verify=False, timeout=60)
                        
                        # éªŒèº«ï¼šå¦‚æœè¿˜æ˜¯ HTMLï¼Œè¯´æ˜ Cookie æ²¡ç”Ÿæ•ˆ
                        content_type = r.headers.get("Content-Type", "").lower()
                        if "html" in content_type and not row['æ–‡ä»¶å'].endswith(".html"):
                            st.toast(f"èº«ä»½éªŒè¯å¤±è´¥(è¿˜æ˜¯ç½‘é¡µ): {row['æ–‡ä»¶å']}", icon="ğŸš«")
                            fail += 1
                            continue
                            
                        zf.writestr(row['æ–‡ä»¶å'], r.content)
                        success += 1
                        my_bar.progress((i + 1) / total)
                    except: fail += 1
            
            my_bar.empty()
            if success > 0:
                progress_text.success(f"âœ… å®Œæˆï¼æˆåŠŸ: {success}, å¤±è´¥: {fail}")
                st.download_button("ğŸš€ ä¿å­˜ ZIP", zip_buffer.getvalue(), "Verified_Files.zip", "application/zip", type="primary")
            else:
                progress_text.error("âš ï¸ å…¨éƒ¨å¤±è´¥ã€‚è¯·æ£€æŸ¥ Cookie æ˜¯å¦è¿‡æœŸæˆ–å¤åˆ¶å®Œæ•´ã€‚")
