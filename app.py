import streamlit as st
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse, unquote
import io
import zipfile
import urllib3
import time
import pandas as pd

# å±è”½è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
st.set_page_config(page_title="OSINT ä¸‹è½½å™¨ (æç®€ç¨³å®šç‰ˆ)", layout="wide", page_icon="ğŸ•µï¸")

# è¾…åŠ©å‡½æ•°
def is_target_file(href):
    valid = ['.pdf', '.xlsx', '.xls', '.csv', '.docx', '.doc', '.zip']
    return any(href.lower().endswith(ext) for ext in valid) or 'download' in href.lower()

def get_file_size_mb(url):
    try:
        r = requests.head(url, verify=False, timeout=5)
        return int(r.headers.get('Content-Length', 0)) / (1024 * 1024)
    except: return 0

# ä¸»ç•Œé¢
st.title("ğŸ•µï¸ OSINT ä¸‹è½½å™¨ (æç®€ç¨³å®šç‰ˆ)")
st.caption("æ— åŒæ­¥ã€æ— è·³è½¬ã€çº¯ç²¹çš„ä¸‹è½½å·¥å…·")

if 'found_files' not in st.session_state: st.session_state['found_files'] = []

# Step 1: æ‰«æ
target_url = st.text_input("ç›®æ ‡ç½‘å€", placeholder="https://...")
if st.button("ğŸš€ æ‰«æ"):
    if target_url:
        try:
            with st.spinner("æ‰«æä¸­..."):
                r = requests.get(target_url, headers={"User-Agent": "Mozilla/5.0"}, verify=False)
                soup = BeautifulSoup(r.text, 'html.parser')
                files = []
                for a in soup.find_all('a', href=True):
                    if is_target_file(a['href']):
                        full_url = urljoin(target_url, a['href'])
                        name = os.path.basename(unquote(urlparse(full_url).path))
                        if '.' not in name[-5:]: name += '.pdf'
                        files.append({"ä¸‹è½½?": False, "åºå·": len(files)+1, "æ–‡ä»¶å": name, "URL": full_url})
                st.session_state['found_files'] = files
                st.success(f"å‘ç° {len(files)} ä¸ªæ–‡ä»¶")
        except Exception as e: st.error(str(e))

# Step 2: ä¸‹è½½
if st.session_state['found_files']:
    st.markdown("---")
    # ç®€å•çš„åŒºé—´é€‰æ‹©
    c1, c2, c3, c4 = st.columns([1,1,2,2])
    start = c1.number_input("èµ·å§‹", 1, value=1)
    end = c2.number_input("ç»“æŸ", 1, value=min(len(st.session_state['found_files']), 30))
    
    if c3.button("âœ… é€‰ä¸­æ­¤èŒƒå›´"):
        for f in st.session_state['found_files']:
            f['ä¸‹è½½?'] = (start <= f['åºå·'] <= end)
    
    if c4.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰"):
        for f in st.session_state['found_files']: f['ä¸‹è½½?'] = False

    # ç®€å•çš„è¡¨æ ¼ (æ— å›è°ƒï¼Œæ— è‡ªåŠ¨åŒæ­¥ï¼Œå› æ­¤ä¸ä¼šè·³)
    df = pd.DataFrame(st.session_state['found_files'])
    edited_df = st.data_editor(df, height=400, key="editor", hide_index=True, 
                               column_config={"URL": st.column_config.LinkColumn()})
    
    # ä¸‹è½½é€»è¾‘
    selected = edited_df[edited_df["ä¸‹è½½?"] == True]
    count = len(selected)
    
    if st.button(f"ğŸ“¦ ä¸‹è½½ ({count} ä¸ªæ–‡ä»¶)", type="primary"):
        if count > 0:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w") as zf, st.status("æ­£åœ¨ä¸‹è½½...") as status:
                for i, row in selected.iterrows():
                    try:
                        sz = get_file_size_mb(row['URL'])
                        if sz > 100: 
                            status.write(f"âš ï¸ è·³è¿‡å¤§æ–‡ä»¶: {row['æ–‡ä»¶å']}")
                            continue
                        status.write(f"ä¸‹è½½: {row['æ–‡ä»¶å']}")
                        r = requests.get(row['URL'], verify=False, timeout=60)
                        zf.writestr(row['æ–‡ä»¶å'], r.content)
                    except: pass
            st.download_button("ğŸš€ ä¿å­˜ ZIP", zip_buffer.getvalue(), "files.zip", "application/zip", type="primary")
